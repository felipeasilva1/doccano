{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioridades:\n",
    "+ Parâmetros utilizados no classificador.\n",
    "\n",
    "+ Analisar o formato dos dados que tem maior acerto e menor acerto tambem.\n",
    "\n",
    "+ Visualização: Separar o conjunto de test em 2 ou 3 arquivos e visualizar o que o modelo classificou e o que os anotadores classificaram (separar por id do anotador).\n",
    "\n",
    "+ Incluir POS tagging.\n",
    "\n",
    "+ Tranformar o dado $x_i$ em um $x'_i$ que incorpora os 2 últimos e próximos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "os.chdir(\"mock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*/**/***/****.{}'.format(extension))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'161704902/[PRATICA_ETAPA_1]/Documentos/20180510_Rcl_22328_314302526.ner.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_filenames[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria uma tag de inicio e fim de arquivo em cada arquivo antes de apendar todos os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for all_files in all_filenames:\n",
    "    df = pd.read_csv(all_files,delimiter=';', na_values='NaN')\n",
    "    df['Tag'].iloc[0] , df['Tag'].iloc[-1] = ['INICIO_ARQ', 'FIM_ARQ']\n",
    "    frames.append(df)\n",
    "    \n",
    "combined_csv = pd.concat(frames)\n",
    "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas dos arquivos concatenados: 967910\n"
     ]
    }
   ],
   "source": [
    "combined_csv.head() , combined_csv.tail()\n",
    "print(\"Número de linhas dos arquivos concatenados:\", len(combined_csv['Tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_csv.reset_index(inplace=True, drop=True)\n",
    "combined_csv = combined_csv.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>967900</th>\n",
       "      <td>Chefe</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967901</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967902</th>\n",
       "      <td>do</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967903</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967904</th>\n",
       "      <td>Plenário</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967905</th>\n",
       "      <td>\\n</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967906</th>\n",
       "      <td>id</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967907</th>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967908</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967909</th>\n",
       "      <td>20141203_ADI_4350_285683668</td>\n",
       "      <td>FIM_ARQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Token      Tag\n",
       "967900                        Chefe        O\n",
       "967901                                     O\n",
       "967902                           do        O\n",
       "967903                                     O\n",
       "967904                     Plenário        O\n",
       "967905                           \\n        O\n",
       "967906                           id        O\n",
       "967907                            :        O\n",
       "967908                                     O\n",
       "967909  20141203_ADI_4350_285683668  FIM_ARQ"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_csv[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontra parágrafo duplo no arquivo. Uma opção de separar por sentenças."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padrões(sentenças) encontrados: 9708\n"
     ]
    }
   ],
   "source": [
    "a_df = combined_csv #Simplifica o nome do arquivo \n",
    "starts = a_df[a_df['Token']=='\\n'].index & a_df[a_df['Token'].shift(-1)=='\\n'].index #Identifica os paragrafos duplos\n",
    "print(u'Padrões(sentenças) encontrados:', len(starts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_csv.iloc[:starts[0]+2] # Primeira sentença\n",
    "# combined_csv.iloc[starts[-1]+2:] # Última sentença"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 43s, sys: 94.3 ms, total: 5min 43s\n",
      "Wall time: 5min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     Token         Tag  Sentence #\n",
       " 0   Ementa  INICIO_ARQ  Sentence 1\n",
       " 1                    O  Sentence 1\n",
       " 2        e           O  Sentence 1\n",
       " 3                    O  Sentence 1\n",
       " 4  Acórdão           O  Sentence 1,\n",
       "                               Token      Tag     Sentence #\n",
       " 967905                           \\n        O  Sentence 9709\n",
       " 967906                           id        O  Sentence 9709\n",
       " 967907                            :        O  Sentence 9709\n",
       " 967908                                     O  Sentence 9709\n",
       " 967909  20141203_ADI_4350_285683668  FIM_ARQ  Sentence 9709)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "combined_csv['Sentence #'] = 'Sentence'\n",
    "\n",
    "combined_csv['Sentence #'][:starts[0]+2] = 'Sentence %d'%(1) # Primeira sentença\n",
    "combined_csv['Sentence #'][starts[-1]+2:] = 'Sentence %d'%(len(starts)+1) # Última sentença\n",
    "\n",
    "for i in range(1,len(starts)):\n",
    "    combined_csv['Sentence #'][starts[i-1]+2:starts[i]+2] = 'Sentence %d'%(i+1) \n",
    "\n",
    "combined_csv.head(), combined_csv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9709"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_csv['Sentence #'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Sentence #</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ementa</td>\n",
       "      <td>INICIO_ARQ</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acórdão</td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token         Tag  Sentence #\n",
       "0   Ementa  INICIO_ARQ  Sentence 1\n",
       "1                    O  Sentence 1\n",
       "2        e           O  Sentence 1\n",
       "3                    O  Sentence 1\n",
       "4  Acórdão           O  Sentence 1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_csv.groupby('Tag').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B_Doutrina',\n",
       " 'B_Doutrinador',\n",
       " 'B_Pessoa',\n",
       " 'B_Precedente',\n",
       " 'B_Ref. Legislativa',\n",
       " 'FIM_ARQ',\n",
       " 'INICIO_ARQ',\n",
       " 'I_Doutrina',\n",
       " 'I_Doutrinador',\n",
       " 'I_Pessoa',\n",
       " 'I_Precedente',\n",
       " 'I_Ref. Legislativa']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classes = np.unique(combined_csv.Tag.values).tolist()\n",
    "new_classes.remove('O')\n",
    "new_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Sentence #</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ementa</td>\n",
       "      <td>INICIO_ARQ</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acórdão</td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token         Tag  Sentence #\n",
       "0   Ementa  INICIO_ARQ  Sentence 1\n",
       "1                    O  Sentence 1\n",
       "2        e           O  Sentence 1\n",
       "3                    O  Sentence 1\n",
       "4  Acórdão           O  Sentence 1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv['Token'] = combined_csv['Token'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa as frases para criar contexto na aprendizagem\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s['Token'].values.tolist(),\n",
    "                                                     s['Tag'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('Sentence #').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try: \n",
    "            s = self.grouped['Sentence {}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s \n",
    "        except:\n",
    "            pass\n",
    "#             return None\n",
    "    \n",
    "getter = SentenceGetter(combined_csv)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ementa', 'INICIO_ARQ'), (' ', 'O'), ('e', 'O'), (' ', 'O'), ('Acórdão', 'O'), (' ', 'O'), ('06/03/2018', 'O'), (' ', 'O'), ('PRIMEIRA', 'O'), (' ', 'O'), ('TURMA', 'O'), (' ', 'O'), ('\\n', 'O'), ('\\n', 'O')]\n"
     ]
    }
   ],
   "source": [
    "sent = getter.get_next()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "#     postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "#         postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "#         postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Sentence #</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ementa</td>\n",
       "      <td>INICIO_ARQ</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acórdão</td>\n",
       "      <td>O</td>\n",
       "      <td>Sentence 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token         Tag  Sentence #\n",
       "0   Ementa  INICIO_ARQ  Sentence 1\n",
       "1                    O  Sentence 1\n",
       "2        e           O  Sentence 1\n",
       "3                    O  Sentence 1\n",
       "4  Acórdão           O  Sentence 1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ementa', 'INICIO_ARQ'), (' ', 'O'), ('e', 'O'), (' ', 'O'), ('Acórdão', 'O'), (' ', 'O'), ('06/03/2018', 'O'), (' ', 'O'), ('PRIMEIRA', 'O'), (' ', 'O'), ('TURMA', 'O'), (' ', 'O'), ('\\n', 'O'), ('\\n', 'O')]\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(combined_csv)\n",
    "\n",
    "sent = getter.get_next()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 59s, sys: 284 ms, total: 1min 59s\n",
      "Wall time: 1min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8251934845002721"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', #Gradient Descent\n",
    "    c1=0.1, # coefficient for L1 penalty\n",
    "    c2=0.1, # coefficient for L2 penalty\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False) # whether to include transitions that are possible, but not observed\n",
    "\n",
    "# crf = sklearn_crfsuite.CRF( # Performa 'pior'\n",
    "#     algorithm='l2sgd', # Stochastic Gradient Descent with L2 regularization term\n",
    "#     c2=0.1,\n",
    "#     max_iterations=100,\n",
    "#     all_possible_transitions=True) # whether to include transitions that are possible, but not observed\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=new_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        B_Doutrina      0.825     0.525     0.642        99\n",
      "     B_Doutrinador      0.597     0.425     0.497        87\n",
      "          B_Pessoa      0.703     0.644     0.672       807\n",
      "      B_Precedente      0.839     0.744     0.789       913\n",
      "B_Ref. Legislativa      0.807     0.717     0.760       892\n",
      "           FIM_ARQ      1.000     0.678     0.808        59\n",
      "        INICIO_ARQ      1.000     0.690     0.816        58\n",
      "        I_Doutrina      0.956     0.656     0.778      4225\n",
      "     I_Doutrinador      0.704     0.903     0.791      3565\n",
      "          I_Pessoa      0.841     0.775     0.807      4701\n",
      "      I_Precedente      0.868     0.831     0.849     13490\n",
      "I_Ref. Legislativa      0.871     0.838     0.854     12040\n",
      "\n",
      "         micro avg      0.848     0.805     0.826     40936\n",
      "         macro avg      0.834     0.702     0.755     40936\n",
      "      weighted avg      0.855     0.805     0.825     40936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(y_test, y_pred, labels = new_classes, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-ba94f4064ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_features_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# crf.tagger_.dump(filename=\"crf_tagger.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn_crfsuite/estimator.py\u001b[0m in \u001b[0;36mtransition_features_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn_crfsuite/estimator.py\u001b[0m in \u001b[0;36m_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_cached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_cached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpycrfsuite/_pycrfsuite.pyx\u001b[0m in \u001b[0;36mpycrfsuite._pycrfsuite.Tagger.info\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpycrfsuite/_pycrfsuite.pyx\u001b[0m in \u001b[0;36mpycrfsuite._pycrfsuite.Tagger.info\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pycrfsuite/_dumpparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parse_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_FILEHEADER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pycrfsuite/_dumpparser.py\u001b[0m in \u001b[0;36mparse_STATE_FEATURES\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_STATE_FEATURES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\(\\d+\\) (.*) --> (.*): ([+-]?\\d+\\.\\d+)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "crf.transition_features_\n",
    "# crf.tagger_.dump(filename=\"crf_tagger.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-6500630f0218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top likely transitions:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint_transitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_features_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTop unlikely transitions:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn_crfsuite/estimator.py\u001b[0m in \u001b[0;36mtransition_features_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_from\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn_crfsuite/estimator.py\u001b[0m in \u001b[0;36m_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_cached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_cached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpycrfsuite/_pycrfsuite.pyx\u001b[0m in \u001b[0;36mpycrfsuite._pycrfsuite.Tagger.info\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpycrfsuite/_pycrfsuite.pyx\u001b[0m in \u001b[0;36mpycrfsuite._pycrfsuite.Tagger.info\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pycrfsuite/_dumpparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parse_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_FILEHEADER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pycrfsuite/_dumpparser.py\u001b[0m in \u001b[0;36mparse_STATE_FEATURES\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_STATE_FEATURES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\(\\d+\\) (.+) --> (.+): ([+-]?\\d+\\.\\d+)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "# Let’s check what classifier learned\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(5))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(y_test)):\n",
    "    df = pd.DataFrame(zip(X_test[i], y_test[i], y_pred[i]))\n",
    "    result.append(df)\n",
    "    \n",
    "result = pd.concat(result)\n",
    "result.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns = ['X_teste','y_teste', 'y_Previsto']\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0][0]['word.lower()'], y_train[0][0]\n",
    "# result.X_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Proporção de erros:',result[result.y_teste != result.y_Previsto].shape[0] / result.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(y_test)\n",
    "y_test[0] + y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_novo = []\n",
    "for i in range(len(y_test)):\n",
    "    y_novo += y_test[i]\n",
    "y_novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_novo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(y_test[i]) for i in range(len(y_test))])\n",
    "# len(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "combined_csv.iloc[starts[i-1]+2:starts[i]+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "\n",
    "•algorithm(str, optional (default='lbfgs'))  –  Training  algorithm.   Al-lowed values:–'lbfgs'- Gradient descent using the L-BFGS method–'l2sgd'- Stochastic Gradient Descent with L2 regularization term–'ap'- Averaged Perceptron–'pa'- Passive Aggressive (PA)–'arow'- Adaptive Regularization Of Weight Vector (AROW)\n",
    "\n",
    "•min_freq(float, optional (default=0)) – Cut-off threshold for occurrencefrequency of a feature.  CRFsuite will ignore features whose frequencies of occurrences inthe training data are no greater thanmin_freq. The default is no cut-off.\n",
    "\n",
    "•all_possible_states(bool, optional (default=False))    –    Specifywhether CRFsuite generates state features that do not even occur in the training data (i.e.,negative state features).  When True, CRFsuite generates state features that associate all ofpossible combinations between attributes and labels.Suppose that the numbers of attributes and labels are A and L respectively, this function willgenerate (A * L) features. Enabling this function may improve the labeling accuracy becausethe CRF model can learn the condition where an item is not predicted to its reference label.However, this function may also increase the number of features and slow down the trainingprocess drastically. This function is disabled by default.\n",
    "\n",
    "•all_possible_transitions(bool, optional (default=False)) – Spec-ify whether CRFsuite generates transition features that do not even occur in the training data(i.e., negative transition features).  When True, CRFsuite generates transition features thatassociate all of possible label pairs. Suppose that the number of labels in the training data isL, this function will generate (L * L) transition features. This function is disabled by default.\n",
    "\n",
    "•c1(float, optional (default=0)) – The coefficient for L1 regularization.  If anon-zero value is specified, CRFsuite switches to the Orthant-Wise Limited-memory Quasi-Newton (OWL-QN) method. The default value is zero (no L1 regularization).Supported training algorithms: lbfgs\n",
    "\n",
    "•c2(float, optional (default=1.0)) – The coefficient for L2 regularization.Supported training algorithms: l2sgd, lbfgs\n",
    "\n",
    "•max_iterations(int, optional (default=None)) – The maximum numberof iterations for optimization algorithms. Default value depends on training algorithm:–lbfgs - unlimited;–l2sgd - 1000;1.3.  API Reference13\n",
    "sklearn-crfsuite Documentation, Release 0.3–ap - 100;–pa - 100;–arow - 100.\n",
    "\n",
    "•num_memories(int, optional (default=6)) – The number of limited memo-ries for approximating the inverse hessian matrix.Supported training algorithms: lbfgs•epsilon(float, optional (default=1e-5)) – The epsilon parameter that de-termines the condition of convergence.Supported training algorithms: ap, arow, lbfgs, pa\n",
    "\n",
    "•period(int, optional (default=10)) – The duration of iterations to test thestopping criterion.Supported training algorithms: l2sgd, lbfgs\n",
    "\n",
    "•delta(float, optional (default=1e-5)) – The threshold for the stopping cri-terion; an iteration stops when the improvement of the log likelihood over the lastperioditerations is no greater than this threshold.Supported training algorithms: l2sgd, lbfgs\n",
    "\n",
    "•linesearch(str, optional (default='MoreThuente')) – The line searchalgorithm used in L-BFGS updates. Allowed values:–'MoreThuente'- More and Thuente’s method;–'Backtracking'- backtracking method with regular Wolfe condition;–'StrongBacktracking'- backtracking method with strong Wolfe condition.Supported training algorithms: lbfgs•max_linesearch(int, optional (default=20)) – The maximum number oftrials for the line search algorithm.Supported training algorithms: lbfgs\n",
    "\n",
    "•calibration_eta(float, optional (default=0.1)) – The initial value oflearning rate (eta) used for calibration.Supported training algorithms: l2sgd\n",
    "\n",
    "•calibration_rate(float, optional (default=2.0))  –  The  rate  of  in-crease/decrease of learning rate for calibration.Supported training algorithms: l2sgd\n",
    "\n",
    "•calibration_samples(int, optional (default=1000)) – The number ofinstances used for calibration. The calibration routine randomly chooses instances no largerthancalibration_samples.Supported training algorithms: l2sgd\n",
    "\n",
    "•calibration_candidates(int, optional (default=10))  –  The  numberof  candidates  of  learning  rate.   The  calibration  routine  terminates  after  findingcalibra-tion_samplescandidates of learning rates that can increase log-likelihood.Supported training algorithms: l2sgd14Chapter 1.  Contents\n",
    "sklearn-crfsuite Documentation, Release 0.3\n",
    "\n",
    "•calibration_max_trials(int, optional (default=20)) – The maximumnumber of trials of learning rates for calibration.  The calibration routine terminates aftertryingcalibration_max_trialscandidate values of learning rates.Supported training algorithms: l2sgd\n",
    "\n",
    "•pa_type(int, optional (default=1))  –  The  strategy  for  updating  featureweights. Allowed values:–0 - PA without slack variables;–1 - PA type I;–2 - PA type II.Supported training algorithms: pa\n",
    "\n",
    "•c(float, optional (default=1)) – Aggressiveness parameter (used only for PA-I and PA-II). This parameter controls the influence of the slack term on the objective func-tion.Supported training algorithms: pa\n",
    "\n",
    "•error_sensitive(bool, optional (default=True)) – If this parameter isTrue,  the optimization routine includes into the objective function the square root of thenumber of incorrect labels predicted by the model.Supported training algorithms: pa\n",
    "\n",
    "•averaging(bool, optional (default=True)) – If this parameter is True, theoptimization routine computes the average of feature weights at all updates in the trainingprocess (similarly to Averaged Perceptron).Supported training algorithms: pa\n",
    "\n",
    "•variance(float, optional (default=1)) – The initial variance of every fea-ture weight.  The algorithm initialize a vector of feature weights as a multivariate Gaussiandistribution with mean 0 and variancevariance.Supported training algorithms: arow\n",
    "\n",
    "•gamma(float, optional (default=1)) – The tradeoff between loss function andchanges of feature weights.Supported training algorithms: arow\n",
    "\n",
    "•verbose(bool, optional (default=False)) – Enable trainer verbose mode.\n",
    "\n",
    "•model_filename(str, optional (default=None))  –  A  path  to  an  existingCRFSuite model. This parameter allows to load and use existing crfsuite models.By  default,  model  files  are  created  automatically  and  saved  in  temporary  locations;  thepreferred way to save/load CRF models is to use pickle (or its alternatives like joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_csv.drop('Tag', axis=1) # Define o conjunto X\n",
    "v = DictVectorizer(sparse=True) # Função que transforma listas de features em vetores\n",
    "X = v.fit_transform(X.to_dict('records')) #Aplica a função de vetorização no conjunto \n",
    "                                          #X que foi colocado no formato 'records' (informa o que preenche cada coluna \n",
    "                                          # da linha i)\n",
    "y = combined_csv.Tag.values # Define o conjunto y\n",
    "\n",
    "classes = np.unique(y) # Define quais serão as classes baseado nos valores únicos da coluna y\n",
    "classes = classes.tolist() # Tranforma as classes de array para lista\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=0) # Divide o conjunto em treino\n",
    "                                                                                            #e teste\n",
    "X_train.shape, y_train.shape # Formato dos dados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
