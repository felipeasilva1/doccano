{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessamento:\n",
    "\n",
    "+ Criar variável que identifique o anotador\n",
    "\n",
    "+ Mover tags que começam com token ' ' (vazio)\n",
    "\n",
    "+ Remover linhas com '\\n' seguidos\n",
    "\n",
    "+ REGEX:\n",
    "    + Garantir letra e números onde tamanho for maior que 1.\n",
    "    + Passar múltiplos símbolos para outra linha. Exemplo:  §3º -->  § \\n 3 \\n º\n",
    "    + Remover pontuação de centenas dos números. Exemplo: 12.200 --> 12200\n",
    "\n",
    "+ Visualização das sentenças com displacy (from spacy import displacy) \n",
    "\n",
    "+ Incluir POS tagging.\n",
    "\n",
    "+ Tranformar o dado $x_i$ em um $x'_i$ que incorpora os 2 últimos e próximos tokens.\n",
    "\n",
    "# Classificador\n",
    "\n",
    "+ Visualização: Separar o conjunto de test em 2 ou 3 arquivos e visualizar o que o modelo classificou e o que os anotadores classificaram (separar por id do anotador).\n",
    "\n",
    "+ Parâmetros utilizados no classificador.\n",
    "\n",
    "+ Analisar o formato dos dados que tem maior acerto e menor acerto tambem.\n",
    "\n",
    "+ Para criar um contexto no erro imprimir 10 palavras antes e depois de dois erros.\n",
    "\n",
    "\n",
    "# Instruções para os anotadores:\n",
    "\n",
    "+ Atentar à marcação de tags que envolve espaço \n",
    "\n",
    "+ Atentar para não incluir espaço no início da Tag\n",
    "\n",
    "# Organização do diretório: \n",
    "\n",
    "+ Manter toda a análise em somente um diretório\n",
    "\n",
    "+ Formato do diretório com os datasets: /resources/dataset/\n",
    "\n",
    "+ Notebooks:\n",
    "    + '01 - Processamento.ipynb'\n",
    "        + Gerar 'treino.csv' e teste.csv' para processamento\n",
    "    + '02 - [CRF].ipynb' - Criando o modelo\n",
    "        + Usar arquivo dos dados preprocessados gerado pelo notebook 1.\n",
    "        + Gerar modelo (xxx.model)\n",
    "    + '03 - Metricas.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import scipy.stats\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontra todos os csv's dentro das pastas de 'mock'\n",
    "\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('NER/*/**/***/****/*****.{}'.format(extension))]\n",
    "\n",
    "# NER/NER_EXPORT/161704902/[PRATICA_ETAPA_1]/Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NER/NER_EXPORT/181300055/[PRATICA_ETAPA_1]/Documentos/20141030_RE_541090_271775268.ner.csv',\n",
       " 'NER/NER_EXPORT/181300055/[PRATICA_ETAPA_1]/Documentos/20180411_HC_138057_314087281.ner.csv',\n",
       " 'NER/NER_EXPORT/181300055/[PRATICA_ETAPA_1]/Documentos/20170921_HC_147683_312770301.ner.csv',\n",
       " 'NER/NER_EXPORT/181300055/[PRATICA_ETAPA_1]/Documentos/20180503_Pet_7074_314257052.ner.csv',\n",
       " 'NER/NER_EXPORT/181300055/[PRATICA_ETAPA_1]/Documentos/20180509_HC_135415_314294988.ner.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma tag de inicio e fim de arquivo em cada 'csv' antes de apendar todos eles.\n",
    "\n",
    "frames = []\n",
    "for all_files in all_filenames:\n",
    "    df = pd.read_csv(all_files,delimiter=';', na_values='NaN') # Lê o arquivo\n",
    "    df['Tag'].iloc[0] , df['Tag'].iloc[-1] = ['INICIO_ARQ', 'FIM_ARQ'] # Altera a primeira e ultima Tag desse csv\n",
    "    frames.append(df) # Adiciona esse dataframe no 'dataframe maior'\n",
    "    \n",
    "combined_csv = pd.concat(frames).reset_index(drop=True)\n",
    "combined_csv.to_csv(\"combined_csv.csv\",index=False,encoding='utf-8') # Cria um arquivo com todas as anotações.\n",
    "combined_csv['Token'] = combined_csv['Token'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(             Token         Tag\n",
       " 0           EMENTA  INICIO_ARQ\n",
       " 1                :           O\n",
       " 2                            O\n",
       " 3   CONSTITUCIONAL           O\n",
       " 4                .           O\n",
       " 5                            O\n",
       " 6       TRIBUTÁRIO           O\n",
       " 7                .           O\n",
       " 8                            O\n",
       " 9          IMPOSTO           O\n",
       " 10                           O\n",
       " 11              DE           O\n",
       " 12                           O\n",
       " 13           RENDA           O\n",
       " 14               .           O,\n",
       "                                  Token       Tag\n",
       " 15446379                                       O\n",
       " 15446380                      dezembro         O\n",
       " 15446381                                       O\n",
       " 15446382                            de         O\n",
       " 15446383                                       O\n",
       " 15446384                          2015         O\n",
       " 15446385                             .         O\n",
       " 15446386                                       O\n",
       " 15446387                            \\n         O\n",
       " 15446388                            \\n         O\n",
       " 15446389                      Ministro  B_Pessoa\n",
       " 15446390                                I_Pessoa\n",
       " 15446391                         TEORI  I_Pessoa\n",
       " 15446392                                I_Pessoa\n",
       " 15446393                      ZAVASCKI  I_Pessoa\n",
       " 15446394                            \\n  I_Pessoa\n",
       " 15446395                            id         O\n",
       " 15446396                             :         O\n",
       " 15446397                                       O\n",
       " 15446398  20160201_HC_132184_308418258   FIM_ARQ)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_csv.head(15), combined_csv.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas dos arquivos concatenados: 15446399\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de linhas dos arquivos concatenados:\", len(combined_csv['Tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15446389</td>\n",
       "      <td>Ministro</td>\n",
       "      <td>B_Pessoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15446390</td>\n",
       "      <td></td>\n",
       "      <td>I_Pessoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15446391</td>\n",
       "      <td>TEORI</td>\n",
       "      <td>I_Pessoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15446392</td>\n",
       "      <td></td>\n",
       "      <td>I_Pessoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15446393</td>\n",
       "      <td>ZAVASCKI</td>\n",
       "      <td>I_Pessoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15446394</td>\n",
       "      <td>\\n</td>\n",
       "      <td>I_Pessoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15446395</td>\n",
       "      <td>id</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15446396</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15446397</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15446398</td>\n",
       "      <td>20160201_HC_132184_308418258</td>\n",
       "      <td>FIM_ARQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Token       Tag\n",
       "15446389                      Ministro  B_Pessoa\n",
       "15446390                                I_Pessoa\n",
       "15446391                         TEORI  I_Pessoa\n",
       "15446392                                I_Pessoa\n",
       "15446393                      ZAVASCKI  I_Pessoa\n",
       "15446394                            \\n  I_Pessoa\n",
       "15446395                            id         O\n",
       "15446396                             :         O\n",
       "15446397                                       O\n",
       "15446398  20160201_HC_132184_308418258   FIM_ARQ"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_csv[-10:] # Conferindo se o index foi resetado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontra parágrafo duplo no arquivo. Uma opção de separar por sentenças."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padrões(sentenças) encontrados: 159578\n"
     ]
    }
   ],
   "source": [
    "a_df = combined_csv #Simplifica o nome do arquivo para a função nao ficar grande demais.\n",
    "starts = a_df[a_df['Token']=='\\n'].index & a_df[a_df['Token'].shift(-1)=='\\n'].index #Identifica os paragrafos duplos\n",
    "print(u'Padrões(sentenças) encontrados:', len(starts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "combined_csv['Sentence #'] = 'Sentence'\n",
    "\n",
    "combined_csv['Sentence #'][:starts[0]+2] = 'Sentence %d'%(1) # Primeira sentença\n",
    "combined_csv['Sentence #'][starts[-1]+2:] = 'Sentence %d'%(len(starts)+1) # Última sentença\n",
    "\n",
    "for i in range(1,len(starts)):\n",
    "    combined_csv['Sentence #'][starts[i-1]+2:starts[i]+2] = 'Sentence %d'%(i+1) \n",
    "\n",
    "combined_csv.head(), combined_csv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de sentenças\n",
    "len(combined_csv['Sentence #'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atualiza a Tag que termina com 'Doutrinador' para 'Doutrina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv.Tag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip o final 'dor' de todo o DataFrame (formato extremo)\n",
    "# combined_csv.Tag = combined_csv.Tag.str.rstrip('dor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices de onde a tag ocorre\n",
    "indx = combined_csv[combined_csv.Tag.str.endswith('Doutrinador')].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rodar apenas uma vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Demorado e custoso\n",
    "for i in range(len(indx)):\n",
    "    combined_csv.Tag.iloc[indx[i]] = combined_csv.Tag.iloc[indx[i]].rstrip('dor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv.Tag.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove enter duplo depois de criar as sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste para ver os casoso onde ocorre enter duplo.\n",
    "for i in range(len(starts)):\n",
    "    print(combined_csv.iloc[starts[i]:starts[i]+2][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um array com as posições a serem retiradas.\n",
    "pos = []\n",
    "for i in range(len(starts)):\n",
    "    pos.append(starts[i])\n",
    "    pos.append(starts[i]+1)\n",
    "pos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove as linhas do dataframe e reseta os índices.\n",
    "combined_csv = combined_csv.drop(pos).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirma se a remoção foi bem sucedida.\n",
    "combined_csv.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mover B_  com Token vazio para linha abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'begins' identifica as situações onde a Tag começa com 'B_' e o Token é vazio, uma situação onde \n",
    "# o anotador começou a marcação de um espaço vazio gerando a inconsistância.\n",
    "\n",
    "begins = combined_csv[(combined_csv['Token']==' ') & \n",
    "                      (combined_csv['Tag'].str.startswith('B_'))].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begins[0], len (begins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Rodar apenas uma vez\n",
    "for i in range(len(begins)):\n",
    "    combined_csv.Tag.iloc[begins[i]+1] = combined_csv.Tag.iloc[begins[i]] #Acertar a Tag do Token para começar\n",
    "                                                                          #sem espaço\n",
    "    combined_csv.Tag.iloc[begins[i]] = 'O' #Marca o espaço vazio como 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i, n = 1, 10\n",
    "combined_csv.iloc[begins[i]-n+7:begins[i]+n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratar marcações que incluem vírgula no final da marcação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste = combined_csv.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_teste.iloc[2].shift(+1)\n",
    "\n",
    "df_teste['Token'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índice das posições onde ocorre o fim da marcação em uma vírgula\n",
    "inx = df_teste[(df_teste.Tag.str.startswith('I')) & (df_teste.Token == ',') & (df_teste.Tag.shift(-1) == 'O')].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conferindo a ocorrência\n",
    "i = 9\n",
    "df_teste.iloc[inx[i]-5:inx[i]+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste = df_teste.drop(inx).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conferindo se funcionou\n",
    "i = 9\n",
    "df_teste.iloc[inx[i]-20:inx[i]+15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv = df_teste.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv[(combined_csv.Tag.str.startswith('B')) & (combined_csv.Token == ' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(combined_csv.Tag.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANTE: Esse último passo é necessário para salvar todas as alterações feitas no preprocessamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o processamento feito nos dados\n",
    "combined_csv.to_csv(\"preprocessados.csv\",index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> $\\color{red}{\\text{TESTES FALHOS}}$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratar caracteres especiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um dataframe teste para não alterar o arquivo principal\n",
    "df_teste = combined_csv.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica os tokens que começam com '§' e são maiores que 1 pois queremos pegar os casos onde\n",
    "# '§' está associado ao numero do paragrafo.\n",
    "espec_carac = list(df_teste[(df_teste.Token.str.startswith('§')) & (df_teste.Token.str.len()>1)].Token.unique())\n",
    "espec_carac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste[df_teste.Token.str.match(espec_carac[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste com duas sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senten = df_teste[df_teste['Sentence #'] == 'Sentence 310'].append(df_teste[df_teste['Sentence #'] == 'Sentence 1589'])\n",
    "\n",
    "df_senten.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_senten)):\n",
    "    if any(carac in df_senten.Token.iloc[i] for carac in espec_carac) == True:\n",
    "        line = df_senten.iloc[i]\n",
    "        splt = list(line.Token)\n",
    "        \n",
    "        if line.Tag.startswith('I_'):\n",
    "            if len(line.Token) == 2:\n",
    "                #Para tamanho 2 temos que transformar uma linha em duas\n",
    "                line0 = line.copy()\n",
    "                line0.Token = splt[0]\n",
    "                line1 = line.copy()\n",
    "                line1.Token = splt[1]\n",
    "                # Transforma em DataFrame\n",
    "                line0 = pd.DataFrame(line0).transpose()\n",
    "                line1 = pd.DataFrame(line1).transpose()\n",
    "\n",
    "                df_senten1 = pd.concat([df_senten.iloc[:i], line0, line1, df_senten.iloc[i+1:]]).reset_index(drop=True)\n",
    "            \n",
    "            if len(line.Token) == 3:\n",
    "                #Para tamanho 3 temos que transformar uma linha em três\n",
    "                line0 = line.copy()\n",
    "                line0.Token = splt[0]\n",
    "                line1 = line.copy()\n",
    "                line1.Token = splt[1]\n",
    "                line2 = line.copy()\n",
    "                line2.Token = splt[2]\n",
    "\n",
    "                # Transforma em DataFrame\n",
    "                line0 = pd.DataFrame(line0).transpose()\n",
    "                line1 = pd.DataFrame(line1).transpose()\n",
    "                line2 = pd.DataFrame(line2).transpose()\n",
    "\n",
    "                df_senten1 = pd.concat([df_senten.iloc[:i], line0, line1, line2, df_senten.iloc[i+1:]]).reset_index(drop=True)\n",
    "\n",
    "        if line.Tag.startswith('B_'):\n",
    "            if len(line.Token) == 2:\n",
    "                #Para tamanho 2 temos que transformar uma linha em duas\n",
    "                line0 = line.copy()\n",
    "                line0.Token = splt[0]\n",
    "                line1 = line.copy()\n",
    "                line1.Token = splt[1]\n",
    "                line1.Tag = line.Tag.replace('B_','I_')\n",
    "\n",
    "                # Transforma em DataFrame\n",
    "                line0 = pd.DataFrame(line0).transpose()\n",
    "                line1 = pd.DataFrame(line1).transpose()\n",
    "\n",
    "                df_senten1 = pd.concat([df_senten.iloc[:i], line0, line1, df_senten.iloc[i+1:]]).reset_index(drop=True)\n",
    "            if len(line.Token) == 3:\n",
    "                #Para tamanho 3 temos que transformar uma linha em três\n",
    "                line0 = line.copy()\n",
    "                line0.Token = splt[0]\n",
    "                line1 = line.copy()\n",
    "                line1.Token = splt[1]\n",
    "                line1.Tag = line.Tag.replace('B_','I_')\n",
    "                line2 = line.copy()\n",
    "                line2.Token = splt[2]\n",
    "                line2.Tag = line.Tag.replace('B_','I_')\n",
    "\n",
    "                # Transforma em DataFrame\n",
    "                line0 = pd.DataFrame(line0).transpose()\n",
    "                line1 = pd.DataFrame(line1).transpose()\n",
    "                line2 = pd.DataFrame(line2).transpose()\n",
    "\n",
    "                df_senten1 = pd.concat([df_senten.iloc[:i], line0, line1, line2, df_senten.iloc[i+1:]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vendo se o novo dataframe (df_senten1) está alterado em relação ao dataframe anterior (df_senten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_senten1.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senten.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para o conjunto de teste (com todas as sentenças)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_teste)):\n",
    "    if any(carac in df_teste.Token.iloc[i] for carac in espec_carac) == True:\n",
    "        line = df_teste.iloc[i]\n",
    "        splt = list(line.Token)\n",
    "        \n",
    "        if line.Tag.startswith('I_'):\n",
    "            if len(line.Token) == 2:\n",
    "                #Para tamanho 2 temos que transformar uma linha em duas\n",
    "                line0 = line.copy()\n",
    "                line0.Token = splt[0]\n",
    "                line1 = line.copy()\n",
    "                line1.Token = splt[1]\n",
    "                # Transforma em DataFrame\n",
    "                line0 = pd.DataFrame(line0).transpose()\n",
    "                line1 = pd.DataFrame(line1).transpose()\n",
    "\n",
    "                df_senten1 = pd.concat([df_teste.iloc[:i], line0, line1, df_teste.iloc[i+1:]]).reset_index(drop=True)\n",
    "            \n",
    "            if len(line.Token) == 3:\n",
    "                #Para tamanho 3 temos que transformar uma linha em três\n",
    "                line0 = line.copy()\n",
    "                line0.Token = splt[0]\n",
    "                line1 = line.copy()\n",
    "                line1.Token = splt[1]\n",
    "                line2 = line.copy()\n",
    "                line2.Token = splt[2]\n",
    "\n",
    "                # Transforma em DataFrame\n",
    "                line0 = pd.DataFrame(line0).transpose()\n",
    "                line1 = pd.DataFrame(line1).transpose()\n",
    "                line2 = pd.DataFrame(line2).transpose()\n",
    "\n",
    "                df_senten1 = pd.concat([df_teste.iloc[:i], line0, line1, line2, df_teste.iloc[i+1:]]).reset_index(drop=True)\n",
    "\n",
    "        if line.Tag.startswith('B_'):\n",
    "            if len(line.Token) == 2:\n",
    "                #Para tamanho 2 temos que transformar uma linha em duas\n",
    "                line0 = line.copy()\n",
    "                line0.Token = splt[0]\n",
    "                line1 = line.copy()\n",
    "                line1.Token = splt[1]\n",
    "                line1.Tag = line.Tag.replace('B_','I_')\n",
    "\n",
    "                # Transforma em DataFrame\n",
    "                line0 = pd.DataFrame(line0).transpose()\n",
    "                line1 = pd.DataFrame(line1).transpose()\n",
    "\n",
    "                df_senten1 = pd.concat([df_teste.iloc[:i], line0, line1, df_teste.iloc[i+1:]]).reset_index(drop=True)\n",
    "            if len(line.Token) == 3:\n",
    "                #Para tamanho 3 temos que transformar uma linha em três\n",
    "                line0 = line.copy()\n",
    "                line0.Token = splt[0]\n",
    "                line1 = line.copy()\n",
    "                line1.Token = splt[1]\n",
    "                line1.Tag = line.Tag.replace('B_','I_')\n",
    "                line2 = line.copy()\n",
    "                line2.Token = splt[2]\n",
    "                line2.Tag = line.Tag.replace('B_','I_')\n",
    "\n",
    "                # Transforma em DataFrame\n",
    "                line0 = pd.DataFrame(line0).transpose()\n",
    "                line1 = pd.DataFrame(line1).transpose()\n",
    "                line2 = pd.DataFrame(line2).transpose()\n",
    "\n",
    "                df_senten1 = pd.concat([df_teste.iloc[:i], line0, line1, line2, df_teste.iloc[i+1:]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senten1[(df_senten1.Token.str.startswith('§')) & (df_senten1.Token.str.len()>1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aparentemente, não está funcionando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove pontuação dos números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um DataFrame onde todas os tokens possuem pontuação \n",
    "df_pont = combined_csv[(combined_csv.Token.str.contains(\"\"\"[.]\"\"\")) & (combined_csv.Token.str.len()>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pont.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens de tamanho maior que 1.\n",
    "df_pont.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_csv.iloc[2722-3:2722+5]\n",
    "df_pont[(df_pont.Tag.str.endswith('Precedente'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pont.Tag.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Não foi possível chegar em um consenso sobre qual pontuação deveria ser removida e como localizá-la no df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso onde 'ADV' aparece ao final do Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No meio do preprocessamento foi identificado tokens que terminam com 'adv' e estão colados ao nome do\n",
    "# advogado referente ao caso.\n",
    "# Tokens que terminan com 'adv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = combined_csv[combined_csv.Token.str.endswith('ADV')]#.reset_index(drop=True)\n",
    "adv.head()#, adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desconfiança que o nome do advogado está agregado à palavra 'adv'\n",
    "\n",
    "adv[adv.Token.str.len() >3]\n",
    "adv_pos = adv[adv.Token.str.len() >3].index.values\n",
    "\n",
    "k = 2 # Vê as linhas antes e depois do k-ésimo ocorrido .\n",
    "combined_csv.iloc[adv_pos[k]-3:adv_pos[k]+3] # Olhando para as linhas anteriores e posteriores o acontecimento.\n",
    "# combined_csv.iloc[adv_pos[0]].Token[-3:] #Separando a parte 'adv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quantidade de vezes que esse caso ocorre em todos os arquivos:\",len(adv_pos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
